"""
Bayesian Matrix Product Operator (Full Structure).

Contains:
- Î¼-MPO: Mean tensor network (standard MPO)
- Î£-MPO: Variation tensor network (doubled structure with 'o' and 'i' indices)
- Prior distributions: Gamma distributions for each mode of Î¼-MPO
- Ï„ distribution: Gamma(Î±, Î²) for noise/regularization
"""

import torch
from tensor.node import TensorNode
from tensor.network import TensorNetwork
from tensor.bmpo import BMPONetwork
from tensor.probability_distributions import (
    GammaDistribution, 
    MultivariateGaussianDistribution,
    ProductDistribution
)
from typing import Dict, List, Optional, Tuple, Union, Set, Any


class BayesianMPO:
    """
    Full Bayesian MPO structure.
    
    Structure:
        - Î¼-MPO: Standard tensor network with blocks like (r1, f1, r2)
        - Î£-MPO: Doubled tensor network with blocks like (r1o, r1i, f1o, f1i, r2o, r2i)
          where contractions happen between 'o' (outer) and 'i' (inner) separately
        - Prior parameters: Gamma distributions for Î¼-MPO modes
        - Ï„: Gamma(Î±, Î²) noise distribution
    
    Example:
        Î¼-block shape: (3, 4, 5) with labels ['r1', 'f1', 'r2']
        Î£-block shape: (3, 3, 4, 4, 5, 5) with labels ['r1o', 'r1i', 'f1o', 'f1i', 'r2o', 'r2i']
    """
    
    def __init__(
        self, 
        mu_nodes: List[TensorNode], 
        input_nodes: Optional[List[TensorNode]] = None, 
        rank_labels: Optional[Union[List[str], set]] = None, 
        tau_alpha: Optional[torch.Tensor] = None, 
        tau_beta: Optional[torch.Tensor] = None
    ) -> None:
        """
        Initialize Bayesian MPO.
        
        Args:
            mu_nodes: List of nodes for Î¼-MPO (standard MPO blocks)
            input_nodes: List of input nodes (optional)
            rank_labels: Set of dimension labels that are ranks (horizontal bonds)
            tau_alpha: Alpha parameter for Ï„ ~ Gamma(Î±, Î²)
            tau_beta: Beta parameter for Ï„ ~ Gamma(Î±, Î²)
        """
        self.input_nodes = input_nodes or []
        self.mu_nodes = mu_nodes
        
        # Create Î¼-MPO network with prior distributions
        self.mu_mpo = BMPONetwork(
            input_nodes=self.input_nodes,
            main_nodes=self.mu_nodes,
            rank_labels=rank_labels
        )
        
        # Create Î£-MPO structure (doubled)
        self.sigma_nodes = self._create_sigma_nodes()
        self.sigma_mpo = TensorNetwork(
            input_nodes=[],  # Î£-MPO doesn't directly connect to inputs
            main_nodes=self.sigma_nodes
        )
        
        # Ï„ distribution: Gamma(Î±, Î²)
        if tau_alpha is None or tau_beta is None:
            # Default values
            self.tau_alpha = torch.tensor(2.0)
            self.tau_beta = torch.tensor(1.0)
        else:
            self.tau_alpha = tau_alpha
            self.tau_beta = tau_beta
        
        self.tau_distribution = GammaDistribution(
            concentration=self.tau_alpha,
            rate=self.tau_beta
        )
        
        # Initialize prior hyperparameters (uninformative by default)
        self._initialize_prior_hyperparameters()
    
    def _create_sigma_nodes(self) -> List[TensorNode]:
        """
        Create Î£-MPO nodes from Î¼-MPO nodes.
        
        Each Î¼-node with shape (d1, d2, d3) and labels ['r1', 'f1', 'r2']
        becomes a Î£-node with shape (d1, d1, d2, d2, d3, d3) 
        and labels ['r1o', 'r1i', 'f1o', 'f1i', 'r2o', 'r2i']
        """
        sigma_nodes = []
        
        for mu_node in self.mu_nodes:
            # Double the shape: each dimension becomes (dim, dim)
            sigma_shape = []
            sigma_labels = []
            sigma_left_labels = []
            sigma_right_labels = []
            
            for dim_size, label in zip(mu_node.shape, mu_node.dim_labels):
                # Create 'o' (outer) and 'i' (inner) versions
                sigma_shape.extend([dim_size, dim_size])
                label_o = f"{label}o"
                label_i = f"{label}i"
                sigma_labels.extend([label_o, label_i])
                
                # Handle left/right bond labels
                if label in mu_node.left_labels:
                    sigma_left_labels.extend([label_o, label_i])
                if label in mu_node.right_labels:
                    sigma_right_labels.extend([label_o, label_i])
            
            # Create Î£-node
            sigma_node = TensorNode(
                tensor_or_shape=tuple(sigma_shape),
                dim_labels=sigma_labels,
                l=sigma_left_labels if sigma_left_labels else None,
                r=sigma_right_labels if sigma_right_labels else None,
                name=f"sigma_{mu_node.name}"
            )
            
            sigma_nodes.append(sigma_node)
        
        # Connect Î£-nodes (outer to outer, inner to inner)
        for i in range(len(sigma_nodes) - 1):
            curr_node = sigma_nodes[i]
            next_node = sigma_nodes[i + 1]
            
            # Find shared bond labels
            for label in curr_node.dim_labels:
                if label in next_node.dim_labels:
                    curr_node.connect(next_node, label)
        
        # Initialize Î£-nodes to ensure positive definite covariance
        self._initialize_sigma_nodes(sigma_nodes)
        
        return sigma_nodes
    
    def _initialize_sigma_nodes(self, sigma_nodes: List[TensorNode]) -> None:
        """
        Initialize Î£-MPO nodes to ensure covariance is positive definite.
        
        For each block, we want Cov = Î£ - Î¼ âŠ— Î¼^T to be positive definite.
        A simple initialization is: Î£ = Î¼ âŠ— Î¼^T + ÏƒÂ²I
        where ÏƒÂ² is a small variance parameter.
        
        This corresponds to setting the Î£-node such that when reshaped:
        E[W âŠ— W^T] = Î¼ âŠ— Î¼^T + ÏƒÂ²I
        
        Args:
            sigma_nodes: List of Î£-MPO nodes to initialize
        """
        for mu_node, sigma_node in zip(self.mu_nodes, sigma_nodes):
            # Get Î¼ block (flattened)
            mu_flat = mu_node.tensor.flatten()
            d = mu_flat.numel()
            
            # Compute Î¼ âŠ— Î¼^T + ÏƒÂ²I
            # Using ÏƒÂ² = 1.0 as default initial variance
            initial_variance = 1.0
            mu_outer = torch.outer(mu_flat, mu_flat)
            sigma_matrix = mu_outer + initial_variance * torch.eye(d, dtype=mu_flat.dtype, device=mu_flat.device)
            
            # Reshape sigma_matrix (d, d) back to Î£-node shape
            # Î£-node has shape (d1, d1, d2, d2, ...) where d = d1 * d2 * ...
            shape_half = mu_node.shape
            n_dims = len(shape_half)
            
            # First reshape to separate dimensions
            # (d, d) -> (d1, d2, ..., d1, d2, ...)
            expanded_shape = list(shape_half) + list(shape_half)
            sigma_expanded = sigma_matrix.reshape(*expanded_shape)
            
            # Permute to interleave outer and inner: (d1, d2, ..., d1, d2, ...) -> (d1, d1, d2, d2, ...)
            # Create permutation that interleaves
            perm = []
            for i in range(n_dims):
                perm.append(i)           # outer dimension i
                perm.append(i + n_dims)  # inner dimension i
            
            sigma_permuted = sigma_expanded.permute(*perm)
            
            # Set the Î£-node tensor
            sigma_node.tensor = sigma_permuted

    def _get_sigma_to_mu_permutation(self, block_idx: int) -> Tuple[List[int], List[int]]:
        """
        Get permutation to reshape Î£-block to match Î¼-block structure using labels.
        
        Î£-node has labels like ['r1o', 'r1i', 'c2o', 'c2i', 'p2o', 'p2i', 'r2o', 'r2i']
        Î¼-node has labels like ['r1', 'c2', 'p2', 'r2']
        
        This function finds which Î£ dimensions correspond to outer and inner for each Î¼ dimension.
        
        Args:
            block_idx: Index of the block
            
        Returns:
            Tuple of (outer_indices, inner_indices) where:
            - outer_indices[i] is the Î£ dimension for Î¼ dimension i (outer)
            - inner_indices[i] is the Î£ dimension for Î¼ dimension i (inner)
            
        Example:
            Î¼ labels: ['r1', 'c2', 'p2', 'r2']
            Î£ labels: ['r1o', 'r1i', 'c2o', 'c2i', 'p2o', 'p2i', 'r2o', 'r2i']
            Returns: ([0, 2, 4, 6], [1, 3, 5, 7])
        """
        mu_node = self.mu_nodes[block_idx]
        sigma_node = self.sigma_nodes[block_idx]
        
        outer_indices = []
        inner_indices = []
        
        for mu_label in mu_node.dim_labels:
            # Find corresponding 'o' and 'i' labels in sigma
            label_o = f"{mu_label}o"
            label_i = f"{mu_label}i"
            
            # Get their indices in sigma
            try:
                idx_o = sigma_node.dim_labels.index(label_o)
                idx_i = sigma_node.dim_labels.index(label_i)
            except ValueError as e:
                raise ValueError(f"Cannot find labels {label_o} or {label_i} in Î£-node labels {sigma_node.dim_labels}")
            
            outer_indices.append(idx_o)
            inner_indices.append(idx_i)
        
        return outer_indices, inner_indices
    
    def _sigma_to_matrix(self, block_idx: int) -> torch.Tensor:
        """
        Convert Î£-block tensor to (d, d) matrix using label-based permutation.
        
        Uses labels to correctly identify outer and inner dimensions, then
        permutes to group them and reshapes to matrix form.
        
        Args:
            block_idx: Index of the block
            
        Returns:
            Matrix of shape (d, d) where d is the flattened size of Î¼-block
        """
        mu_node = self.mu_nodes[block_idx]
        sigma_node = self.sigma_nodes[block_idx]
        
        # Get permutation using labels
        outer_indices, inner_indices = self._get_sigma_to_mu_permutation(block_idx)
        
        # Permute: [outer dims, then inner dims]
        perm = outer_indices + inner_indices
        sigma_permuted = sigma_node.tensor.permute(*perm)
        
        # Reshape to (d, d)
        d = mu_node.tensor.numel()
        sigma_matrix = sigma_permuted.reshape(d, d)
        
        return sigma_matrix

    def _matrix_to_sigma(self, block_idx: int, matrix: torch.Tensor) -> torch.Tensor:
        """
        Convert (d, d) matrix back to Î£-block tensor shape.
        
        This is the inverse operation of _sigma_to_matrix.
        
        Args:
            block_idx: Index of the block
            matrix: Matrix of shape (d, d)
            
        Returns:
            Tensor with Î£-block shape (d1_o, d1_i, d2_o, d2_i, ...)
        """
        mu_node = self.mu_nodes[block_idx]
        sigma_node = self.sigma_nodes[block_idx]
        
        mu_shape = mu_node.shape
        d = mu_node.tensor.numel()
        n_dims = len(mu_shape)
        
        # Reshape matrix (d, d) to grouped form
        # First: (d, d) -> (d1, d2, ..., d1, d2, ...)
        expanded_shape = list(mu_shape) + list(mu_shape)
        matrix_expanded = matrix.reshape(*expanded_shape)
        
        # Get inverse permutation
        # Forward was: outer_indices + inner_indices
        # Inverse: interleave them back
        outer_indices, inner_indices = self._get_sigma_to_mu_permutation(block_idx)
        
        # Create inverse permutation
        # We went from interleaved [o, i, o, i, ...] to grouped [o, o, ..., i, i, ...]
        # Now go back: from grouped to interleaved
        inv_perm = [0] * (2 * n_dims)
        for i, (o_idx, i_idx) in enumerate(zip(outer_indices, inner_indices)):
            inv_perm[o_idx] = i  # Outer goes to position i
            inv_perm[i_idx] = i + n_dims  # Inner goes to position i + n_dims
        
        sigma_tensor = matrix_expanded.permute(*inv_perm)
        
        return sigma_tensor
    
    def _initialize_prior_hyperparameters(self) -> None:
        """
        Initialize prior hyperparameters p(Î¸).
        
        The prior should be set BEFORE variational parameters, as priors inform initialization.
        Default prior hyperparameters (uninformative/weakly informative):
        - p(Ï„) ~ Gamma(Î±â‚€=1, Î²â‚€=1)  - uninformative
        - p(Wáµ¢) ~ N(0, Ïƒâ‚€Â²I) where Ïƒâ‚€Â²=1 - weakly informative
        - p(bond params) ~ Gamma(1, 1) - uninformative, for each index in each bond
        
        These should be set via set_prior_hyperparameters() before running inference.
        """
        # Prior for Ï„: uninformative Gamma(1, 1)
        self.prior_tau_alpha = torch.tensor(1.0, dtype=self.tau_alpha.dtype, device=self.tau_alpha.device)
        self.prior_tau_beta = torch.tensor(1.0, dtype=self.tau_beta.dtype, device=self.tau_beta.device)
        
        # Prior for bond parameters: uninformative Gamma(1, 1) for all bonds
        # Store as dict: {label: {'concentration0': tensor, 'rate0': tensor}}
        # Each bond has N Gamma distributions (one per index)
        self.prior_bond_params = {}
        for label, dist_info in self.mu_mpo.distributions.items():
            size = len(dist_info['expectation'])
            self.prior_bond_params[label] = {
                'concentration0': torch.ones(size, dtype=dist_info['concentration'].dtype, 
                                            device=dist_info['concentration'].device),
                'rate0': torch.ones(size, dtype=dist_info['rate'].dtype, 
                                   device=dist_info['rate'].device)
            }
        
        # Prior for blocks: N(0, Ïƒâ‚€Â²I) with Ïƒâ‚€Â²=1 (weakly informative)
        # OPTIMIZATION: Store only the scalar variance Ïƒâ‚€Â² instead of full matrix
        # This saves memory since the covariance is diagonal: Î£â‚€ = Ïƒâ‚€Â²I
        self.prior_block_sigma0_scalar = []
        self.prior_block_sigma0_isotropic = []  # Flag indicating if prior is isotropic (ÏƒÂ²I)
        
        for mu_node in self.mu_nodes:
            d = mu_node.tensor.numel()
            # Store scalar variance (isotropic prior)
            sigma0_scalar = torch.tensor(1.0, dtype=mu_node.tensor.dtype, device=mu_node.tensor.device)
            self.prior_block_sigma0_scalar.append(sigma0_scalar)
            self.prior_block_sigma0_isotropic.append(True)
        
        # For backward compatibility, also maintain the old interface
        # but compute matrices lazily only when needed
        self.prior_block_sigma0 = None  # Will be computed on demand

    def set_random_prior_hyperparameters(
        self,
        tau_alpha_range: tuple[float, float] = (0.5, 5.0),
        tau_beta_range: tuple[float, float] = (0.5, 5.0),
        bond_concentration_range: tuple[float, float] = (0.5, 5.0),
        bond_rate_range: tuple[float, float] = (0.5, 5.0),
        block_sigma_range: tuple[float, float] = (0.5, 10.0),
        seed: Optional[int] = None
    ) -> 'BayesianMPO':
        """
        Set random prior hyperparameters from uniform distributions.
        
        This creates more diverse priors instead of fixed uninformative priors.
        
        Args:
            tau_alpha_range: Range for p(Ï„) ~ Gamma(Î±â‚€, Î²â‚€) concentration parameter
            tau_beta_range: Range for p(Ï„) ~ Gamma(Î±â‚€, Î²â‚€) rate parameter
            bond_concentration_range: Range for bond parameter Gamma concentrations
            bond_rate_range: Range for bond parameter Gamma rates
            block_sigma_range: Range for block prior variance diagonal values
            seed: Random seed for reproducibility
            
        Returns:
            self for chaining
        """
        if seed is not None:
            torch.manual_seed(seed)
        
        # Random prior for Ï„: Gamma(Î±â‚€, Î²â‚€) with random Î±â‚€, Î²â‚€
        self.prior_tau_alpha = torch.empty_like(self.tau_alpha).uniform_(*tau_alpha_range)
        self.prior_tau_beta = torch.empty_like(self.tau_beta).uniform_(*tau_beta_range)
        
        # Random prior for bond parameters: Gamma with random parameters for each bond index
        for label, dist_info in self.mu_mpo.distributions.items():
            size = len(dist_info['expectation'])
            self.prior_bond_params[label] = {
                'concentration0': torch.empty(size, dtype=dist_info['concentration'].dtype,
                                             device=dist_info['concentration'].device).uniform_(*bond_concentration_range),
                'rate0': torch.empty(size, dtype=dist_info['rate'].dtype,
                                    device=dist_info['rate'].device).uniform_(*bond_rate_range)
            }
        
        # Random prior for blocks: N(0, Ïƒâ‚€Â²I) with random Ïƒâ‚€Â² per block
        # OPTIMIZED: Store only scalar variance
        self.prior_block_sigma0_scalar = []
        self.prior_block_sigma0_isotropic = []
        
        for mu_node in self.mu_nodes:
            # Random variance for each block (isotropic)
            sigma0_scalar = torch.empty(1, dtype=mu_node.tensor.dtype, 
                                       device=mu_node.tensor.device).uniform_(*block_sigma_range).squeeze()
            self.prior_block_sigma0_scalar.append(sigma0_scalar)
            self.prior_block_sigma0_isotropic.append(True)
        
        # Invalidate cached full matrices
        self.prior_block_sigma0 = None
        
        return self
    
    def set_prior_hyperparameters(
        self,
        tau_alpha0: Optional[torch.Tensor] = None,
        tau_beta0: Optional[torch.Tensor] = None,
        bond_params0: Optional[Dict[str, Dict[str, torch.Tensor]]] = None,
        block_sigma0: Optional[List[torch.Tensor]] = None
    ) -> None:
        """
        Set prior hyperparameters for p(Î¸).
        
        Args:
            tau_alpha0: Prior Î±â‚€ for p(Ï„) ~ Gamma(Î±â‚€, Î²â‚€)
            tau_beta0: Prior Î²â‚€ for p(Ï„) ~ Gamma(Î±â‚€, Î²â‚€)
            bond_params0: Dict of prior parameters for bonds, e.g.:
                         {'r1': {'concentration0': tensor, 'rate0': tensor}}
                         Each tensor has length N (one param per index)
            block_sigma0: List of prior covariances Î£â‚€ for p(Wáµ¢) ~ N(0, Î£â‚€)
        """
        if tau_alpha0 is not None:
            self.prior_tau_alpha = tau_alpha0
        if tau_beta0 is not None:
            self.prior_tau_beta = tau_beta0
        if bond_params0 is not None:
            self.prior_bond_params.update(bond_params0)
        if block_sigma0 is not None:
            self.prior_block_sigma0 = block_sigma0
    
    def get_mu_distributions(self, label: str) -> Optional[List[GammaDistribution]]:
        """Get Gamma distributions for a Î¼-MPO dimension."""
        return self.mu_mpo.get_gamma_distributions(label)
    
    def get_mu_expectations(self, label: str) -> Optional[torch.Tensor]:
        """Get expectation values E[Ï‰] or E[Ï†] for Î¼-MPO dimension."""
        return self.mu_mpo.get_expectations(label)
    
    def update_mu_params(
        self, 
        label: str, 
        concentration: Optional[torch.Tensor] = None, 
        rate: Optional[torch.Tensor] = None
    ) -> None:
        """
        Update Î¼-MPO distribution parameters for a bond.
        
        Args:
            label: Bond label
            concentration: Concentration parameters (Î±)
            rate: Rate parameters (Î²)
        """
        self.mu_mpo.update_distribution_params(label, concentration, rate)

    def get_nodes_for_bond(self, label: str) -> Dict[str, List[int]]:
        """
        Get node indices associated with a given bond for both Î¼-MPO and Î£-MPO.
        
        Args:
            label: Bond label (e.g., 'r1', 'f1')
            
        Returns:
            Dictionary with keys:
            - 'mu_nodes': List of Î¼-MPO node indices sharing this bond
            - 'sigma_nodes': List of Î£-MPO node indices sharing this bond (same as mu_nodes)
        """
        mu_node_indices = self.mu_mpo.get_nodes_for_bond(label)
        
        # Î£-MPO nodes correspond 1-to-1 with Î¼-MPO nodes
        # The same node indices apply to both
        sigma_node_indices = mu_node_indices.copy()
        
        return {
            'mu_nodes': mu_node_indices,
            'sigma_nodes': sigma_node_indices
        }

    def compute_theta_tensor(self, block_idx: int, exclude_labels: Optional[List[str]] = None) -> torch.Tensor:
        """
        Compute the Theta tensor for a Î¼-MPO block from Gamma expectations.
        
        Theta_{ijk...} = E_q[bond1]_i Ã— E_q[bond2]_j Ã— E_q[bond3]_k Ã— ...
        
        Args:
            block_idx: Index of the Î¼-MPO block (0 to N-1)
            exclude_labels: List of bond labels to exclude (set to dimension 1)
            
        Returns:
            Theta tensor matching the block shape
            
        Example:
            # Block 1 has bonds ['r1', 'c2', 'p2', 'r2'] with shape (4, 1, 5, 4)
            theta = bmpo.compute_theta_tensor(1)
            # Shape: (4, 1, 5, 4) from outer product of [E[r1], 1, E[p2], E[r2]]
            
            theta_no_p = bmpo.compute_theta_tensor(1, exclude_labels=['p2'])
            # Shape: (4, 1, 1, 4), excludes p2 contribution
        """
        return self.mu_mpo.compute_theta_tensor(block_idx, exclude_labels)

    def partial_trace_update(self, block_idx: int, focus_label: str) -> torch.Tensor:
        """
        Compute partial trace for variational update of a specific bond.
        
        This computes: Î£_{other indices} [diag(Î£) + Î¼Â²] Ã— Î˜_{without focus_label}
        
        where the sum is over all dimensions except focus_label, and the multiplication
        aligns matching labels.
        
        Algorithm:
        1. Get diagonal of Î£-block: diag(Î£_{ijk...} with paired (io, ii))
        2. Add Î¼Â² element-wise: v = diag(Î£) + Î¼Â²
        3. Get Î˜ excluding focus_label
        4. Multiply: v Ã— Î˜ (element-wise, matching dimensions)
        5. Sum over all dimensions except focus_label
        
        Args:
            block_idx: Index of the block
            focus_label: The bond label to keep (not sum over)
            
        Returns:
            Vector of length equal to the dimension of focus_label
            
        Example:
            Block has shape (4, 1, 5, 4) with labels ['r1', 'c2', 'p2', 'r2']
            partial_trace_update(1, 'p2') returns vector of length 5
            
            This computes: Î£_{i,j,k} [diag(Î£) + Î¼Â²]_{ijkð“} Ã— Î˜_{ijkð“ without p2}
            where the sum is over r1(i), c2(j), r2(k), keeping only p2(ð“)
        """
        mu_node = self.mu_nodes[block_idx]
        
        # Check that focus_label is in this block
        if focus_label not in mu_node.dim_labels:
            raise ValueError(f"Label '{focus_label}' not found in block {block_idx}. "
                           f"Available labels: {mu_node.dim_labels}")
        
        # Step 1: Extract diagonal of Î£-block using label-based method
        sigma_matrix = self._sigma_to_matrix(block_idx)  # Shape: (d, d)
        diag_sigma_flat = torch.diagonal(sigma_matrix)  # Shape: (d,)
        
        # Reshape back to block shape
        mu_shape = mu_node.shape
        diag_sigma = diag_sigma_flat.reshape(mu_shape)
        
        # Step 2: Add Î¼Â² element-wise
        mu_tensor = mu_node.tensor
        v = diag_sigma + mu_tensor ** 2  # Shape: same as mu_shape
        
        # Step 3: Get Î˜ excluding focus_label
        theta = self.compute_theta_tensor(block_idx, exclude_labels=[focus_label])
        # Theta has shape where focus_label dimension is 1
        
        # Step 4: Multiply element-wise (broadcasting handles the dimension of size 1)
        result = v * theta  # Shape: same as mu_shape
        
        # Step 5: Sum over all dimensions except focus_label
        # Find which dimension index corresponds to focus_label
        focus_dim_idx = mu_node.dim_labels.index(focus_label)
        
        # Sum over all dimensions except focus_dim_idx
        dims_to_sum = [i for i in range(len(mu_shape)) if i != focus_dim_idx]
        output = torch.sum(result, dim=dims_to_sum)
        
        return output

    def update_bond_variational(self, label: str) -> None:
        """
        Variational update for a bond's Gamma distribution parameters.
        
        Updates q(bond) ~ Gamma(concentration_q, rate_q) based on the current
        Î¼ and Î£ values.
        
        Update formulas:
        - concentration_q = concentration_p + N_b Ã— dim(bond)
        - rate_q = rate_p - Î£_{blocks with bond} partial_trace_update(block, label)
        
        where:
        - N_b is the number of learnable blocks associated with the bond (1 or 2)
        - dim(bond) is the dimension size of the bond
        - concentration_p, rate_p are prior hyperparameters
        - partial_trace_update contracts over all dimensions except the bond
        
        Args:
            label: Bond label to update (e.g., 'r1', 'p2')
            
        Example:
            # Bond 'r1' connects blocks [0, 1], has dimension 4
            bmpo.update_bond_variational('r1')
            
            # Updates:
            # concentration_q = concentration_p + 2 Ã— 4 = concentration_p + 8
            # rate_q = rate_p - partial_trace(block=0, 'r1') 
            #                 - partial_trace(block=1, 'r1')
        """
        # Check that the bond exists and has Gamma distribution
        if label not in self.mu_mpo.distributions:
            raise ValueError(f"Bond '{label}' not found in distributions. "
                           f"Available bonds: {list(self.mu_mpo.distributions.keys())}")
        
        # Get bond information
        bond_dist = self.mu_mpo.distributions[label]
        bond_size = len(bond_dist['concentration'])
        
        # Get blocks associated with this bond
        nodes_info = self.get_nodes_for_bond(label)
        block_indices = nodes_info['mu_nodes']
        N_b = len(block_indices)  # Number of blocks (1 or 2)
        
        # Get prior parameters
        if label not in self.prior_bond_params:
            raise ValueError(f"No prior parameters found for bond '{label}'")
        
        prior_params = self.prior_bond_params[label]
        concentration_p = prior_params['concentration0']  # Shape: (bond_size,)
        rate_p = prior_params['rate0']  # Shape: (bond_size,)
        
        # Update formula 1: concentration_q = concentration_p + N_b Ã— dim(bond)
        concentration_q = concentration_p + N_b * bond_size
        
        # Update formula 2: rate_q = rate_p - Î£_{blocks} partial_trace_update
        rate_q = rate_p.clone()
        
        for block_idx in block_indices:
            # Compute partial trace for this block with focus on the bond
            partial_trace = self.partial_trace_update(block_idx, label)
            # Shape: (bond_size,)
            
            # ADD to rate_q
            rate_q = rate_q + partial_trace
        
        # Update the bond parameters
        self.update_mu_params(label, concentration=concentration_q, rate=rate_q)

    def _compute_mu_jacobian_outer(
        self,
        node: TensorNode,
        network: 'TensorNetwork',
        output_shape: tuple
    ) -> torch.Tensor:
        """
        Compute Î£â‚™ J_Î¼(xâ‚™) âŠ— J_Î¼(xâ‚™) - outer product of Jacobian with itself.
        
        This is J^T @ J summed over samples, without hessian weighting.
        """
        from tensor.utils import EinsumLabeler
        
        # Get Jacobian
        J = network.compute_jacobian_stack(node).copy()
        
        # Expand to include output labels
        J = J.expand_labels(network.output_labels, output_shape)
        
        broadcast_dims = tuple(d for d in network.output_labels if d not in node.dim_labels)
        J = J.permute_first(*broadcast_dims)
        
        # Build einsum for J^T @ J
        dim_labels = EinsumLabeler()
        
        J_ein1 = ''.join([dim_labels[d] for d in J.dim_labels])
        J_ein2 = ''.join([dim_labels['_' + d] if d != network.sample_dim else dim_labels[d] for d in J.dim_labels])
        
        # Output: node dimensions twice
        J_out1 = []
        J_out2 = []
        dim_order = []
        for d in J.dim_labels:
            if d not in broadcast_dims:
                J_out1.append(dim_labels[d])
                J_out2.append(dim_labels['_' + d])
                dim_order.append(d)
        
        out1 = ''.join([J_out1[dim_order.index(d)] for d in node.dim_labels])
        out2 = ''.join([J_out2[dim_order.index(d)] for d in node.dim_labels])
        
        # Einsum: sum over samples, outer product over node dims
        einsum_str = f'{J_ein1},{J_ein2}->{out1}{out2}'
        
        A = torch.einsum(einsum_str, J.tensor.conj(), J.tensor)
        
        return A
    
    def _compute_forward_without_node(
        self,
        node: TensorNode,
        network: 'TensorNetwork'
    ) -> TensorNode:
        """
        Compute forward pass through network without contracting a specific node.
        
        This gives us the Jacobian: when contracted with the node, produces the output.
        Uses left and right stacks for efficient computation.
        
        Args:
            node: Node to exclude from contraction
            network: TensorNetwork (mu_mpo or sigma_mpo)
            
        Returns:
            Jacobian as TensorNode
        """
        # Get left and right stacks
        if network.left_stacks is None or network.right_stacks is None:
            network.recompute_all_stacks()
        
        left_stack, right_stack = network.get_stacks(node)
        
        # Get column nodes (vertical connections)
        column_nodes = network.get_column_nodes(node)
        
        # Start with left stack (or first column node if no left stack)
        node_iter = iter(column_nodes)
        contracted = next(node_iter) if left_stack is None else left_stack
        
        # Contract with remaining column nodes
        for vnode in node_iter:
            contracted = contracted.contract_with(vnode, vnode.get_connecting_labels(contracted))
        
        # Contract with right stack
        if right_stack is not None:
            contracted = contracted.contract_with(right_stack, right_stack.get_connecting_labels(contracted))
        
        return contracted
    
    def _compute_jacobian_y_contraction(
        self,
        node: TensorNode,
        y: torch.Tensor,
        network: 'TensorNetwork'
    ) -> torch.Tensor:
        """
        Compute Î£â‚™ yâ‚™ Â· J(xâ‚™) where J is the Jacobian (forward without node).
        
        This contracts J with y over the sample dimension, leaving node dimensions.
        """
        from tensor.utils import EinsumLabeler
        
        # Get Jacobian: network output without this node
        J = self._compute_forward_without_node(node, network)
        
        # J has sample dimension 's' and dimensions from other parts of network
        # We need to contract J with y over samples, leaving only node-shaped output
        
        # Build einsum string
        labeler = EinsumLabeler()
        
        # J einsum string
        J_ein = ''.join([labeler[d] for d in J.dim_labels])
        
        # y einsum string (sample dimension)
        y_ein = labeler['s']  # y has shape (S,) or (S, 1)
        
        # Output: only dimensions that are in the node
        out_dims = []
        for d in J.dim_labels:
            if d != 's' and d in node.dim_labels:
                out_dims.append(d)
        out_ein = ''.join([labeler[d] for d in out_dims])
        
        # Contract: sum over samples and any non-node dimensions
        einsum_str = f"{J_ein},{y_ein}->{out_ein}"
        
        result = torch.einsum(einsum_str, J.tensor, y.squeeze(-1) if y.dim() > 1 else y)
        
        return result
    
    def _compute_jacobian_outer_product(
        self,
        node: TensorNode,
        network: 'TensorNetwork'
    ) -> torch.Tensor:
        """
        Compute Î£â‚™ J(xâ‚™) âŠ— J(xâ‚™) where J is the Jacobian.
        
        This computes J^T @ J contracted over samples, leaving (node_dims, node_dims).
        """
        from tensor.utils import EinsumLabeler
        
        # Get Jacobian
        J = self._compute_forward_without_node(node, network)
        
        # Build einsum for outer product
        labeler = EinsumLabeler()
        
        # J einsum strings (same J used twice with different labels for outer product)
        J_ein1 = ''.join([labeler[d] for d in J.dim_labels])
        J_ein2 = ''.join([labeler['_' + d] if d != 's' else labeler[d] for d in J.dim_labels])
        
        # Output: node dimensions twice (outer product)
        out_dims1 = [d for d in J.dim_labels if d != 's' and d in node.dim_labels]
        out_dims2 = ['_' + d for d in out_dims1]
        out_ein = ''.join([labeler[d] for d in out_dims1]) + ''.join([labeler[d] for d in out_dims2])
        
        # Contract
        einsum_str = f"{J_ein1},{J_ein2}->{out_ein}"
        
        result = torch.einsum(einsum_str, J.tensor, J.tensor)
        
        return result
    
    def update_block_variational(
        self, 
        block_idx: int, 
        X: torch.Tensor, 
        y: torch.Tensor
    ) -> None:
        """
        Variational update for a block's parameters (Î¼ and Î£).
        
        Updates the natural parameters for block Ï‡â± based on data (X, y).
        
        Update equations:
        - Î¼^Ï‡â± = E[Ï„] Î£^Ï‡â± Î£â‚™ yâ‚™ Â· J_Î¼(xâ‚™)
        - Î£^Ï‡â±â»Â¹ = diag(Î˜) + E[Ï„] Î£â‚™ [J_Î£(xâ‚™) + J_Î¼(xâ‚™) âŠ— J_Î¼(xâ‚™)]
        
        where:
        - J_Î¼(xâ‚™) = âˆ‚/âˆ‚Ï‡â± Î¼(xâ‚™) is the Î¼-Jacobian (network with block i removed)
        - J_Î£(xâ‚™) = âˆ‚/âˆ‚Ï‡â± Î£(xâ‚™âŠ—xâ‚™) is the Î£-Jacobian
        - Î˜ = theta tensor (outer product of Gamma expectations)
        - diag(Î˜) = diagonal matrix from Î˜
        
        Args:
            block_idx: Index of the block to update (0 to N-1)
            X: Input data, shape (S, feature_dims) where S is number of samples
            y: Target data, shape (S,) for scalar outputs
            
        Example:
            # Update block 1 with training data
            bmpo.update_block_variational(1, X_train, y_train)
        """
        mu_node = self.mu_nodes[block_idx]
        sigma_node = self.sigma_nodes[block_idx]
        
        # Get E[Ï„]
        E_tau = self.get_tau_mean()
        
        # Flatten block shape
        d = mu_node.tensor.numel()
        mu_shape = mu_node.shape
        
        # Get Theta tensor (prior precision diagonal)
        theta = self.compute_theta_tensor(block_idx)  # Shape: mu_shape
        theta_flat = theta.flatten()  # Shape: (d,) - just keep as vector
        
        # Forward pass through Î¼-MPO to set up network state with all samples
        mu_output = self.forward_mu(X, to_tensor=False)
        
        # Set output_labels to match actual output
        self.mu_mpo.output_labels = tuple(mu_output.dim_labels)
        
        # Prepare y with proper shape to match output dimensions
        # Output has shape (S, r0, r2) so y should be (S, 1, 1)
        y_expanded = y
        for _ in range(len(mu_output.shape) - 1):  # Add dims for each non-sample dim
            y_expanded = y_expanded.unsqueeze(-1)
        
        # Compute J_Î¼ terms
        # For the b term: Î£â‚™ yâ‚™ Â· J_Î¼(xâ‚™)
        sum_y_dot_J_mu = self.mu_mpo.get_b(mu_node, y_expanded)
        
        # For the A term: Î£â‚™ J_Î¼(xâ‚™) âŠ— J_Î¼(xâ‚™) - outer product without hessian
        J_mu_outer = self._compute_mu_jacobian_outer(mu_node, self.mu_mpo, mu_output.shape)
        
        # Flatten results
        sum_y_dot_J_mu_flat = sum_y_dot_J_mu.flatten()  # Shape: (d,)
        n_dims = len(mu_shape)
        J_mu_outer_flat = J_mu_outer.flatten(0, n_dims-1).flatten(1, -1)  # Shape: (d, d)
        
        # Compute J_Î£ term for Î£-MPO
        # J_Î£ is just the Jacobian contracted over samples: Î£â‚™ J_Î£(xâ‚™)
        # This is the b term from get_b for Î£-MPO
        sigma_output = self.forward_sigma(X, to_tensor=False)
        self.sigma_mpo.output_labels = tuple(sigma_output.dim_labels)
        
        # Create y_sigma matching sigma output shape (all ones to get Jacobian sum)
        y_sigma = torch.ones(sigma_output.shape, dtype=mu_node.tensor.dtype, device=mu_node.tensor.device)
        
        # Get J_Î£ term using get_b - returns tensor with Î£-node shape
        J_sigma_sum = self.sigma_mpo.get_b(sigma_node, y_sigma)
        
        # Convert J_Î£ from Î£-block shape to (d, d) matrix using existing method
        # J_sigma_sum has shape like (d1o, d1i, d2o, d2i, ...)
        # We need to reshape it to (d, d) where d = d1*d2*...
        # Use the same logic as _sigma_to_matrix but for the Jacobian
        sigma_matrix = self._sigma_to_matrix(block_idx)  # Get current Î£ as (d, d)
        # J_sigma_sum should have same structure, reshape it the same way
        # Actually J_sigma_sum already has the right shape, just need to flatten properly
        
        # Get permutation to convert to matrix form
        outer_indices, inner_indices = self._get_sigma_to_mu_permutation(block_idx)
        perm = outer_indices + inner_indices
        J_sigma_permuted = J_sigma_sum.permute(*perm)
        sum_J_sigma = J_sigma_permuted.reshape(d, d)
        
        # Compute covariance inverse: Î£^â»Â¹ = diag(Î˜) + E[Ï„] Î£â‚™ [J_Î£(xâ‚™) + J_Î¼(xâ‚™) âŠ— J_Î¼(xâ‚™)]
        # Compute data term first
        sigma_inv = E_tau * (sum_J_sigma + J_mu_outer_flat)
        
        # Add diagonal terms: diag(Î˜)
        # Avoid creating full diagonal matrix, just add to diagonal
        sigma_inv.diagonal().add_(theta_flat)
        
        # Compute covariance: Î£ = (Î£^â»Â¹)^â»Â¹
        sigma_cov = torch.inverse(sigma_inv)
        
        # Compute mean: Î¼ = E[Ï„] Î£ Î£â‚™ yâ‚™ Â· J_Î¼(xâ‚™)
        mu_flat = E_tau * torch.matmul(sigma_cov, sum_y_dot_J_mu_flat)
        
        # Reshape back to block shape
        mu_new = mu_flat.reshape(mu_shape)
        
        # Update Î¼-block
        mu_node.tensor = mu_new
        
        # Update Î£-block
        # Convert sigma_cov (d, d) back to Î£-block shape
        sigma_new = self._matrix_to_sigma(block_idx, sigma_cov)
        sigma_node.tensor = sigma_new
        
        # Reset stacks after update
        self.mu_mpo.reset_stacks()
        self.sigma_mpo.reset_stacks()
    
    def update_tau(
        self, 
        alpha: Optional[torch.Tensor] = None, 
        beta: Optional[torch.Tensor] = None
    ) -> None:
        """
        Update Ï„ distribution parameters.
        
        Args:
            alpha: New Î± parameter
            beta: New Î² parameter
        """
        if alpha is not None:
            self.tau_alpha = alpha
        if beta is not None:
            self.tau_beta = beta
        
        self.tau_distribution = GammaDistribution(
            concentration=self.tau_alpha,
            rate=self.tau_beta
        )

    def compute_elbo(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        # TODO: WARNING: Elbo diverges to - inf.
        # We need to debug this, in such a way:
        # -find where it explodes. (Also it should increase stadily.)
        # -slowly go back from the return printing all the values to pinpoint exactly which part of the computation is causing the issue!
        """
        Compute the Evidence Lower Bound (ELBO).
        
        ELBO = E_q[log p(y|Î¸,Ï„)] + E_q[log p(Î¸)] + E_q[log p(Ï„)] - E_q[log q(Î¸)] - E_q[log q(Ï„)]
        
        Note: E_q[log p(Î¸)] includes E_q[log p(Ï„)], so we don't add it separately
        
        Args:
            X: Input data, shape (S, feature_dims)
            y: Target data, shape (S,)
            
        Returns:
            ELBO value (scalar tensor)
        """
        S = X.shape[0]
        
        # Forward passes
        mu_output = self.forward_mu(X, to_tensor=True)  # (S, ...)
        sigma_output = self.forward_sigma(X, to_tensor=True)  # (S, ...)
        
        # Type assertions
        assert isinstance(mu_output, torch.Tensor)
        assert isinstance(sigma_output, torch.Tensor)
        
        # Flatten to (S,)
        mu_pred = mu_output.reshape(S, -1).squeeze(-1)  # (S,)
        sigma_pred = sigma_output.reshape(S, -1).squeeze(-1)  # (S,)
        
        # E[Ï„] and E[log Ï„]
        E_tau = self.get_tau_mean()
        E_log_tau = torch.digamma(self.tau_alpha) - torch.log(self.tau_beta)
        
        # Term 1: E_q[log p(y|Î¸,Ï„)]
        # log p(y|Î¸,Ï„) = -1/2 * log(2Ï€) + 1/2 * log(Ï„) - Ï„/2 * (y - f)Â²
        # Taking expectation over q: E_q[(y-f)Â²] = yÂ² - 2y*E_q[f] + E_q[fÂ²]
        # where E_q[fÂ²] = Var[f] + E[f]Â² = Î£(x) + Î¼(x)Â²
        # Therefore: E_q[log p(y|Î¸,Ï„)] = -S/2 * log(2Ï€) + S/2 * E[log Ï„] - E[Ï„]/2 * Î£[yÂ² - 2y*Î¼ + Î£ + Î¼Â²]
        # TODO: isnt this already computed in the rate update of tau? is it needed to be recomputed? since we compute elbo after tau ate update cant we just use that?
        y_squared = y ** 2
        y_times_mu = y * mu_pred
        mu_squared = mu_pred ** 2
        log_likelihood = (
            -0.5 * S * torch.log(torch.tensor(2 * torch.pi, dtype=X.dtype, device=X.device))
            + 0.5 * S * E_log_tau
            - 0.5 * E_tau * (y_squared.sum() - 2.0 * y_times_mu.sum() + sigma_pred.sum() + mu_squared.sum())
        )
        
        # Term 2: E_q[log p(Î¸)] - prior on blocks, bonds, and Ï„
        # NOTE: This already includes E_q[log p(Ï„)]
        log_prior_theta = self.compute_expected_log_prior()
        
        # Term 3: -E_q[log q(Î¸)] - entropy of blocks and bonds
        entropy_theta = torch.tensor(0.0, dtype=X.dtype, device=X.device)
        
        # Block entropies (Gaussian)
        # TODO: isnt already a method for the class of gaussian? using the q distribution definition for the block? simply passing the mu block and sigma block? check for it and if ti works the same!
        for block_idx in range(len(self.mu_nodes)):
            sigma_matrix = self._sigma_to_matrix(block_idx)
            d = sigma_matrix.shape[0]
            sign, logdet = torch.slogdet(sigma_matrix)
            entropy_block = 0.5 * (d * (1 + torch.log(torch.tensor(2 * torch.pi, dtype=X.dtype, device=X.device))) + logdet)
            entropy_theta += entropy_block
        
        # Bond entropies (Gamma)
        # TODO: isnt implemented a product probability to do this iteration under the hood? cant it be applied to this? shouldnt that be defined at the start or is it too complicated?
        for label in self.mu_mpo.distributions.keys():
            # Get Gamma distributions for this bond
            gammas = self.mu_mpo.get_gamma_distributions(label)
            if gammas is not None:
                for gamma in gammas:
                    entropy_theta += gamma.entropy()
        
        # Term 4: -E_q[log q(Ï„)] - entropy of Ï„ (Gamma)
        entropy_tau = self.get_tau_entropy()
        
        # ELBO = likelihood + prior + entropy
        # Note: log_prior_theta already includes E_q[log p(Ï„)]
        elbo = log_likelihood + log_prior_theta + entropy_theta + entropy_tau
        
        return elbo
    
    def fit(
        self,
        X: torch.Tensor,
        y: torch.Tensor,
        max_iter: int = 100,
        tol: float = 1e-6,
        trim_threshold: Optional[float] = None,
        verbose: bool = True
    ) -> None:
        """
        Fit the Bayesian MPO using coordinate ascent variational inference.
        
        Update routine:
        1. Update all blocks (Î¼, Î£ parameters)
        2. Update all bond distributions (Gamma parameters)
        3. Update Ï„ (precision parameter)
        4. Trim (optional)
        5. Repeat until convergence
        
        Args:
            X: Input data, shape (S, feature_dims)
            y: Target data, shape (S,)
            max_iter: Maximum number of iterations
            tol: Convergence tolerance (not implemented yet - always runs max_iter)
            trim_threshold: If provided, trim bonds with E[X] < threshold after each iteration
            verbose: Print progress
            
        Example:
            bmpo.fit(X_train, y_train, max_iter=50, verbose=True)
        """
        if verbose:
            print('='*70)
            print('BAYESIAN MPO COORDINATE ASCENT')
            print('='*70)
            print(f'Data: {X.shape[0]} samples, {X.shape[1]} features')
            print(f'Blocks: {len(self.mu_nodes)}')
            print(f'Max iterations: {max_iter}')
            if trim_threshold is not None:
                print(f'Trim threshold: {trim_threshold}')
            print('='*70)
            print()
        # TODO: track also the r2
        mse = float('nan')
        elbo = float('nan')
        
        for iteration in range(max_iter):
            if verbose:
                print(f'Iteration {iteration + 1}/{max_iter}')
            
            # Step 1: Update all blocks (Î¼, Î£)
            if verbose:
                print('  Updating blocks...', end='')
            for block_idx in range(len(self.mu_nodes)):
                self.update_block_variational(block_idx, X, y)
            if verbose:
                print(' âœ“')
            
            # Step 2: Update all bond distributions (Gamma parameters)
            if verbose:
                print('  Updating bonds...', end='')
            # Get all bond labels (rank labels)
            bond_labels = list(self.mu_mpo.distributions.keys())
            for label in bond_labels:
                self.update_bond_variational(label)
            if verbose:
                print(f' âœ“ ({len(bond_labels)} bonds)')
            
            # Step 3: Update Ï„ (precision)
            if verbose:
                print('  Updating Ï„...', end='')
            self.update_tau_variational(X, y)
            tau_mean = self.get_tau_mean().item()
            if verbose:
                print(f' âœ“ (E[Ï„] = {tau_mean:.4f})')
            
            # Step 4: Compute MSE and ELBO
            if verbose:
                print('  Computing metrics...', end='')
            mu_pred = self.forward_mu(X, to_tensor=True)
            assert isinstance(mu_pred, torch.Tensor)
            mse = torch.mean((mu_pred.reshape(-1) - y.reshape(-1)) ** 2).item()
            
            try:
                elbo = self.compute_elbo(X, y).item()
                if verbose:
                    print(f' âœ“ (MSE = {mse:.4f}, ELBO = {elbo:.2f})')
            except Exception as e:
                if verbose:
                    print(f' âœ“ (MSE = {mse:.4f}, ELBO = failed)')
                elbo = float('nan')
            
            # Step 5: Trim (optional)
            if trim_threshold is not None:
                if verbose:
                    print('  Trimming...', end='')
                # Build threshold dict for all bonds
                trim_thresholds = {label: trim_threshold for label in bond_labels}
                self.trim(trim_thresholds)
                if verbose:
                    print(' âœ“')
            
            if verbose:
                print()
        
        if verbose:
            print('='*70)
            print('TRAINING COMPLETE')
            print(f'Final MSE: {mse:.4f}')
            if not (isinstance(elbo, float) and elbo != elbo):  # Check if not NaN
                print(f'Final ELBO: {elbo:.2f}')
            print('='*70)
    
    def update_tau_variational(
        self,
        X: torch.Tensor,
        y: torch.Tensor
    ) -> None:
        """
        Variational update for Ï„ parameters (coordinate ascent).
        
        Updates q(Ï„) ~ Gamma(Î±_q, Î²_q) based on data (X, y).
        
        Update formulas:
        - Î±_q = Î±_p + S/2
        - Î²_q = Î²_p + 1/2 * Î£_s[y_sÂ² - 2*y_sÂ·Î¼(x_s) + Î¼(x_s)Â² + Î£(x_s)]
        
        This simplifies from the expected squared residual:
        E[(y - f)Â²] = yÂ² - 2yÂ·E[f] + E[fÂ²] = yÂ² - 2yÂ·Î¼ + (Î¼Â² + Î£)
        
        where:
        - S is the number of samples
        - Î±_p, Î²_p are prior hyperparameters
        - Î¼(x_s) is Î¼-MPO forward pass for sample s (scalar output)
        - Î£(x_s) is Î£-MPO forward pass for sample s (scalar output)
        
        Args:
            X: Input data, shape (S, feature_dims) where S is number of samples
            y: Target data, shape (S,) or (S, 1) for scalar outputs
        """
        S = X.shape[0]  # Number of samples
        
        # Update Î±_q = Î±_p + S/2
        alpha_q = self.prior_tau_alpha + S / 2.0
        
        # Compute Î²_q = Î²_p + Î£_s[y_s Â· Î¼(x_s)] + 1/2 * Î£_s[Î£(x_s)]
        
        # Term 1: Î²_p (prior)
        beta_q = self.prior_tau_beta.clone()
        
        # Forward pass through Î¼-MPO for all samples at once
        mu_output_result = self.forward_mu(X, to_tensor=True)  # Returns Tensor when to_tensor=True
        # Type narrowing: when to_tensor=True, result is Tensor
        assert isinstance(mu_output_result, torch.Tensor)
        mu_flat = mu_output_result.reshape(S, -1)  # (S, output_dim)
        y_flat = y.reshape(S, -1)  # (S, output_dim)
        
        # Forward pass through Î£-MPO for all samples at once
        sigma_output_result = self.forward_sigma(X, to_tensor=True)  # Returns Tensor when to_tensor=True
        # Type narrowing: when to_tensor=True, result is Tensor
        assert isinstance(sigma_output_result, torch.Tensor)
        
        # Squeeze Î£ output to remove dummy dimensions and match Î¼ shape
        sigma_flat = sigma_output_result.reshape(S, -1)  # (S, output_dim)
        
        # Compute Î²_q = Î²_p + 0.5 * Î£[yÂ² - 2yÂ·Î¼ + Î¼Â² + Î£]
        # This is the expected squared residual: E[(y - f)Â²] = yÂ² - 2yÂ·Î¼ + E[fÂ²]
        # where E[fÂ²] = Î¼Â² + Î£
        y_squared = y_flat ** 2
        y_times_mu = y_flat * mu_flat
        mu_squared = mu_flat ** 2
        
        # Sum all terms with 0.5 factor
        beta_q = beta_q + 0.5 * torch.sum(
            y_squared - 2.0 * y_times_mu + mu_squared + sigma_flat
        )
        
        # Update the parameters
        self.update_tau(alpha=alpha_q, beta=beta_q)

    def get_tau_mean(self) -> torch.Tensor:
        """Get mean of Ï„ distribution: E[Ï„] = Î±/Î²."""
        return self.tau_distribution.mean()
    
    def get_tau_entropy(self) -> torch.Tensor:
        """Get entropy of Ï„ distribution."""
        return self.tau_distribution.entropy()
    
    def forward_mu(self, x: torch.Tensor, to_tensor: bool = False) -> Union[TensorNode, torch.Tensor]:
        """
        Forward pass through Î¼-MPO.
        
        Args:
            x: Input data
            to_tensor: If True, return tensor instead of TensorNode
            
        Returns:
            Output from Î¼-MPO
        """
        return self.mu_mpo.forward(x, to_tensor=to_tensor)
    
    def forward_sigma(self, x: Optional[torch.Tensor] = None, to_tensor: bool = False) -> Union[TensorNode, torch.Tensor]:
        """
        Forward pass through Î£-MPO.
        
        Î£-MPO represents the correlation/variation structure E[f(X) âŠ— f(X)^T].
        If x is provided, contracts Î£-MPO with input x (sets both 'o' and 'i' inputs to x).
        If x is None, uses current input node values.
        
        Args:
            x: Input data (optional). If provided, both outer and inner inputs are set to x.
            to_tensor: If True, return tensor instead of TensorNode
            
        Returns:
            Output from Î£-MPO (correlation structure)
        """
        return self.sigma_mpo.forward(x, to_tensor=to_tensor)
    
    def get_mu_jacobian(self, node: TensorNode):
        """
        Get Jacobian for a Î¼-MPO node.
        
        Args:
            node: Î¼-MPO node
            
        Returns:
            Jacobian (network contracted without this node)
        """
        return self.mu_mpo.get_jacobian(node)
    
    def get_sigma_jacobian(self, node: TensorNode):
        """
        Get Jacobian for a Î£-MPO node.
        
        Args:
            node: Î£-MPO node
            
        Returns:
            Jacobian (network contracted without this node)
        """
        return self.sigma_mpo.compute_jacobian_stack(node)
    
    def trim(self, thresholds: Dict[str, float]) -> 'BayesianMPO':
        """
        Trim both Î¼-MPO and Î£-MPO based on Î¼ expectations.
        
        When trimming Î¼-MPO dimension 'r1' to keep indices [0, 2],
        Î£-MPO dimensions 'r1o' and 'r1i' are both trimmed to keep [0, 2].
        
        Args:
            thresholds: Dict mapping dim_label -> threshold value
        """
        # First, determine which indices to keep from Î¼-MPO
        keep_indices = {}
        
        for label, dist in self.mu_mpo.distributions.items():
            threshold = thresholds.get(label, float('-inf'))
            expectations = dist['expectation']
            
            mask = expectations >= threshold
            indices = torch.where(mask)[0]
            
            if len(indices) == 0:
                raise ValueError(
                    f"Trimming dimension '{label}' with threshold {threshold} "
                    f"would remove all indices."
                )
            
            keep_indices[label] = indices
        
        # Trim Î¼-MPO
        self.mu_mpo.trim(thresholds)
        
        # Trim Î£-MPO (both 'o' and 'i' versions)
        sigma_keep_indices = {}
        for label, indices in keep_indices.items():
            sigma_keep_indices[f"{label}o"] = indices
            sigma_keep_indices[f"{label}i"] = indices
        
        for node in self.sigma_nodes:
            self._trim_sigma_node(node, sigma_keep_indices)
        
        # IMPORTANT: Update prior_bond_params to match trimmed dimensions
        # TODO: I dont get here, the prior paramaters stay the same in value, we just remove the trimmed one, is that what you are doing?
        for label, indices in keep_indices.items():
            if label in self.prior_bond_params:
                old_conc = self.prior_bond_params[label]['concentration0']
                old_rate = self.prior_bond_params[label]['rate0']
                
                # Select only the kept indices
                self.prior_bond_params[label]['concentration0'] = torch.index_select(old_conc, 0, indices)
                self.prior_bond_params[label]['rate0'] = torch.index_select(old_rate, 0, indices)
        
        # Reset stacks
        self.sigma_mpo.reset_stacks()
        
        return self
    
    def _trim_sigma_node(self, node: TensorNode, keep_indices: Dict[str, torch.Tensor]) -> None:
        """Trim a Î£-MPO node based on keep_indices."""
        new_tensor = node.tensor
        
        for dim_idx, label in enumerate(node.dim_labels):
            if label in keep_indices:
                indices = keep_indices[label]
                new_tensor = torch.index_select(new_tensor, dim_idx, indices)
        
        node.tensor = new_tensor
    
    def to(
        self, 
        device: Optional[torch.device] = None, 
        dtype: Optional[torch.dtype] = None
    ) -> 'BayesianMPO':
        """Move all structures to device/dtype."""
        self.mu_mpo.to(device=device, dtype=dtype)
        self.sigma_mpo.to(device=device, dtype=dtype)
        if device is not None or dtype is not None:
            self.tau_alpha = self.tau_alpha.to(device=device, dtype=dtype)
            self.tau_beta = self.tau_beta.to(device=device, dtype=dtype)
        self.tau_distribution = GammaDistribution(
            concentration=self.tau_alpha,
            rate=self.tau_beta
        )
        
        # Initialize prior hyperparameters (same structure as q, but separate values)
        # These act as the prior p(Î¸) with the same factorization as q(Î¸)
        self._initialize_prior_hyperparameters()
        return self
    
    def get_block_q_distribution(self, block_idx: int) -> MultivariateGaussianDistribution:
        """
        Get the q-distribution for a specific block as a Multivariate Normal.
        
        For block i, q(W_i) = N(Î¼_i, Î£_i - Î¼_i âŠ— Î¼_i^T)
        where:
        - Î¼_i = E_q[W_i] is the Î¼-MPO block (flattened)
        - Î£_i = E_q[W_i âŠ— W_i^T] is related to the Î£-MPO block (flattened)
        - Covariance = Î£_i - Î¼_i âŠ— Î¼_i^T
        
        Args:
            block_idx: Index of the block (0 to N-1)
            
        Returns:
            MultivariateGaussianDistribution for this block
        """
        mu_node = self.mu_nodes[block_idx]
        
        # Flatten Î¼ block: E_q[W_i]
        mu_flat = mu_node.tensor.flatten()
      
        # Convert Î£ block to (d, d) matrix using label-based permutation
        # Î£-block stores the covariance (variance), not the second moment
        sigma_matrix = self._sigma_to_matrix(block_idx)
        
        # Î£-block already stores the covariance
        covariance = sigma_matrix
        
        # Make symmetric to handle numerical errors (floating point precision)
        # TODO: EXPENSIVE:
        # This is expensive, it should naturally be symmetric is it necessary to do this? Check before and after if there is any changes
        covariance = 0.5 * (covariance + covariance.T)
        
        return MultivariateGaussianDistribution(
            loc=mu_flat,
            covariance_matrix=covariance
        )
    
    def get_all_block_q_distributions(self) -> List[MultivariateGaussianDistribution]:
        """
        Get q-distributions for all blocks.
        
        Returns:
            List of MultivariateGaussianDistribution, one for each block
        """
        return [self.get_block_q_distribution(i) for i in range(len(self.mu_nodes))]
    
    def get_mode_q_distributions(self) -> Dict[str, List[GammaDistribution]]:
        """
        Get all Gamma q-distributions for each mode (dimension).
        
        For each mode (dimension label), we have a list of Gamma distributions,
        one for each index in that dimension.
        
        Returns:
            Dict mapping dimension label -> list of GammaDistribution objects
        """
        mode_distributions = {}
        for label in self.mu_mpo.distributions.keys():
            gammas = self.mu_mpo.get_gamma_distributions(label)
            if gammas is not None:
                mode_distributions[label] = gammas
        return mode_distributions
    
    def get_full_q_distribution(self) -> ProductDistribution:
        """
        Get the full q-distribution as a product of all components.
        
        q(Î¸) = q(Ï„) Ã— âˆ_i q(W_i) Ã— âˆ_modes âˆ_indices q(mode_param)
        
        where:
        - q(Ï„) ~ Gamma(Î±, Î²)
        - q(W_i) ~ N(Î¼_i, Î£_i - Î¼_i âŠ— Î¼_i^T) for each block i
        - q(mode_param) ~ Gamma(c, e) or Gamma(f, g) for each mode index
        
        Returns:
            ProductDistribution containing all component distributions
        """
        all_distributions = []
        
        # 1. Add Ï„ distribution
        all_distributions.append(self.tau_distribution)
        
        # 2. Add all block distributions (Multivariate Normals)
        block_dists = self.get_all_block_q_distributions()
        all_distributions.extend(block_dists)
        
        # 3. Add all mode distributions (Gammas)
        mode_dists = self.get_mode_q_distributions()
        for label, gamma_list in mode_dists.items():
            all_distributions.extend(gamma_list)
        
        return ProductDistribution(all_distributions)
    
    def sample_from_q(self, n_samples: int = 1) -> Dict[str, Any]:
        """
        Sample from the q-distribution.
        
        Args:
            n_samples: Number of samples to generate
            
        Returns:
            Dictionary containing:
            - 'tau': Samples from Ï„ distribution
            - 'blocks': List of samples from each block distribution
            - 'modes': Dict of samples from each mode distribution
        """
        samples = {}
        
        # Sample Ï„
        samples['tau'] = self.tau_distribution.forward().sample((n_samples,))
        
        # Sample blocks
        block_dists = self.get_all_block_q_distributions()
        samples['blocks'] = [
            dist.forward().sample((n_samples,)) for dist in block_dists
        ]
        
        # Sample modes
        mode_dists = self.get_mode_q_distributions()
        samples['modes'] = {}
        for label, gamma_list in mode_dists.items():
            samples['modes'][label] = [
                gamma.forward().sample((n_samples,)) for gamma in gamma_list
            ]
        
        return samples
    
    def log_q(self, theta: Dict[str, Any]) -> torch.Tensor:
        """
        Compute log q(Î¸) for given parameter values.
        
        Args:
            theta: Dictionary containing:
                - 'tau': Ï„ values
                - 'blocks': List of block parameter values (flattened)
                - 'modes': Dict of mode parameter values
                
        Returns:
            log q(Î¸) = log q(Ï„) + Î£ log q(W_i) + Î£ log q(mode_params)
        """
        log_prob = torch.tensor(0.0, dtype=self.tau_alpha.dtype)
        
        # Add log q(Ï„)
        log_prob = log_prob + self.tau_distribution.forward().log_prob(theta['tau'])
        
        # Add log q(W_i) for each block
        block_dists = self.get_all_block_q_distributions()
        for i, (dist, block_val) in enumerate(zip(block_dists, theta['blocks'])):
            log_prob = log_prob + dist.forward().log_prob(block_val)
        
        # Add log q(mode_params)
        mode_dists = self.get_mode_q_distributions()
        for label, gamma_list in mode_dists.items():
            mode_values = theta['modes'][label]
            for gamma, val in zip(gamma_list, mode_values):
                log_prob = log_prob + gamma.forward().log_prob(val)
        
        return log_prob
    
    def compute_expected_log_prior(self) -> torch.Tensor:
        """
        Compute E_q[log p(Î¸)] where p(Î¸) is the prior.
        
        The prior factorizes as:
        p(Î¸) = p(Ï„) Ã— âˆáµ¢ p(Wáµ¢) Ã— âˆ_modes âˆ_indices p(mode_param)
        
        Therefore:
        E_q[log p(Î¸)] = E_q[log p(Ï„)] + Î£áµ¢ E_q[log p(Wáµ¢)] + Î£_modes Î£_indices E_q[log p(mode_param)]
        
        Returns:
            E_q[log p(Î¸)] as a scalar tensor
        """
        log_p_total = torch.tensor(0.0, dtype=self.tau_alpha.dtype, device=self.tau_alpha.device)
        
        # 1. E_q[log p(Ï„)]
        log_p_tau = self._expected_log_prior_tau()
        log_p_total = log_p_total + log_p_tau
        
        # 2. E_q[log p(Wáµ¢)] for each block
        for i in range(len(self.mu_nodes)):
            log_p_block = self._expected_log_prior_block(i)
            log_p_total = log_p_total + log_p_block
        
        # 3. E_q[log p(mode_params)] for all modes
        log_p_modes = self._expected_log_prior_modes()
        log_p_total = log_p_total + log_p_modes
        
        return log_p_total
    
    def _expected_log_prior_tau(self) -> torch.Tensor:
        """
        Compute E_q[log p(Ï„)] where p(Ï„) ~ Gamma(Î±â‚€, Î²â‚€).
        
        Uses the modular expected_log_prob method from GammaDistribution.
        
        Returns:
            E_q[log p(Ï„)]
        """
        # Simply call the distribution's method with prior parameters
        return self.tau_distribution.expected_log_prob(
            concentration_p=self.prior_tau_alpha,
            rate_p=self.prior_tau_beta
        )
    
    def _expected_log_prior_block(self, block_idx: int) -> torch.Tensor:
        """
        Compute E_q[log p(Wáµ¢)] where p(Wáµ¢) ~ N(0, Î£â‚€).
        
        OPTIMIZED: For isotropic prior Î£â‚€ = Ïƒâ‚€Â²I, avoids creating full matrix.
        
        Args:
            block_idx: Index of the block
            
        Returns:
            E_q[log p(Wáµ¢)]
        """
        # Get the q-distribution for this block
        q_block = self.get_block_q_distribution(block_idx)
        
        # Prior mean is 0
        mu_q = q_block.loc
        d = mu_q.shape[0]
        
        # Get q's covariance
        sigma_q = q_block.covariance_matrix
        
        # OPTIMIZATION: For isotropic prior Î£â‚€ = Ïƒâ‚€Â²I, compute efficiently
        if self.prior_block_sigma0_isotropic[block_idx]:
            sigma0_sq = self.prior_block_sigma0_scalar[block_idx]
            
            # TODO: WARNING: this equation is wrong!
            # 
            # E_q[log p] = - d*0.5*log 2 \pi + 0.5 * log | \sigma_p^-1| - 0.5 * trace((\sigma_q + \mu_q \outer \mu_q)\sigma_p^-1)
            # Implement this, and indeed keep into account that \sigma_p is isotropic, but actually it could also be different, but still diagonal.
            # check if it is already implemented like this or equivalent
            
            trace_term = torch.trace(sigma_q) / sigma0_sq
            mahalanobis_term = torch.sum(mu_q ** 2) / sigma0_sq
            logdet_sigma_p = d * torch.log(sigma0_sq)
            
            result = (-0.5 * d * torch.log(torch.tensor(2 * torch.pi, dtype=mu_q.dtype, device=mu_q.device))
                     - 0.5 * logdet_sigma_p
                     - 0.5 * (trace_term + mahalanobis_term))
            
            return result
        else:
            # Fall back to full matrix computation for non-isotropic priors
            # (For backward compatibility if prior_block_sigma0 is set manually)
            if self.prior_block_sigma0 is None:
                # Build full matrices on demand
                self.prior_block_sigma0 = []
                for i, sigma_scalar in enumerate(self.prior_block_sigma0_scalar):
                    d_i = self.mu_nodes[i].tensor.numel()
                    if self.prior_block_sigma0_isotropic[i]:
                        sigma_full = sigma_scalar * torch.eye(d_i, dtype=self.mu_nodes[i].tensor.dtype,
                                                               device=self.mu_nodes[i].tensor.device)
                    else:
                        raise NotImplementedError("Non-isotropic priors not yet supported in optimized code")
                    self.prior_block_sigma0.append(sigma_full)
            
            sigma_p = self.prior_block_sigma0[block_idx]
            mu_p = torch.zeros_like(mu_q)
            
            return q_block.expected_log_prob(
                loc_p=mu_p,
                covariance_matrix_p=sigma_p
            )
    
    def _expected_log_prior_modes(self) -> torch.Tensor:
        """
        Compute E_q[log p(bond_params)] for all bonds.
        
        Uses the modular expected_log_prob method from GammaDistribution.
        Each bond has N Gamma distributions (one per index).
        
        Returns:
            Sum of E_q[log p(bond_param)] over all bonds and indices
        """
        log_p_bonds_total = torch.tensor(0.0, dtype=self.tau_alpha.dtype, device=self.tau_alpha.device)
        
        for label, prior_params in self.prior_bond_params.items():
            # Get current variational distributions
            gammas_q = self.mu_mpo.get_gamma_distributions(label)
            if gammas_q is None:
                continue
            
            # Get prior parameters (unified structure)
            concentration0 = prior_params['concentration0']
            rate0 = prior_params['rate0']
            
            # Compute E_q[log p(Î¸)] for each index in this bond using modular method
            for i, gamma_q in enumerate(gammas_q):
                log_p_theta = gamma_q.expected_log_prob(
                    concentration_p=concentration0[i],
                    rate_p=rate0[i]
                )
                log_p_bonds_total = log_p_bonds_total + log_p_theta
        
        return log_p_bonds_total
    
    def summary(self) -> None:
        """Print summary of the Bayesian MPO structure."""
        print("=" * 70)
        print("Bayesian MPO Summary")
        print("=" * 70)
        
        print(f"\nÎ¼-MPO:")
        print(f"  Number of nodes: {len(self.mu_nodes)}")
        for i, node in enumerate(self.mu_nodes):
            print(f"  Node {i}: {node.shape} with labels {node.dim_labels}")
        
        print(f"\nÎ£-MPO (doubled structure):")
        print(f"  Number of nodes: {len(self.sigma_nodes)}")
        for i, node in enumerate(self.sigma_nodes):
            print(f"  Node {i}: {node.shape} with labels {node.dim_labels}")
        
        print(f"\nÏ„ distribution: Gamma(Î±={self.tau_alpha.item():.2f}, Î²={self.tau_beta.item():.2f})")
        print(f"  E[Ï„] = {self.get_tau_mean():.4f}")
        print(f"  H[Ï„] = {self.get_tau_entropy():.4f}")
        
        print(f"\nPrior distributions (Î¼-MPO):")
        for label, dist in self.mu_mpo.distributions.items():
            print(f"  {label}: {dist['type']} ({dist['variable']}), size={len(dist['expectation'])}")
